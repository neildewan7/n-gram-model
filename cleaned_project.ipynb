{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"project.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Language Models ðŸ—£\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dissecting the Corpus ðŸªšðŸ«€\n",
    "\n",
    "<a name='part1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book(url):\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    book = response.text.replace('\\r\\n', '\\n')\n",
    "\n",
    "    start = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
    "    \n",
    "    start_indx = book.find(start)\n",
    "    \n",
    "    start_indx = book.find(\"***\", start_indx + len(start)) + 3\n",
    "\n",
    "    end = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "    \n",
    "    end_indx = book.find(end)\n",
    "\n",
    "    return book[start_indx:end_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "beowulf = get_book('https://www.gutenberg.org/ebooks/16328.txt.utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q1 results: All test cases passed!"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(book_string):\n",
    "    # Strip leading and trailing whitespace\n",
    "    book_string = book_string.strip()\n",
    "    \n",
    "    # Replace two or more newlines with paragraph end and start markers\n",
    "    book_string = re.sub(r'\\n{2,}', '\\x03\\x02', book_string)\n",
    "    \n",
    "    # Add start and end markers to the entire text\n",
    "    book_string = '\\x02' + book_string + '\\x03'\n",
    "    \n",
    "    # Split the text into tokens using regex\n",
    "    tokens = re.findall(r'\\x02|\\x03|[\\w]+|[^\\w\\s]', book_string)\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to prepare the autograder tests. It runs your `tokenize` function on the full body of Shakespeare's work. As a guide, you should expect the first three elements of `shakes` to be `'\\x02'`, `'The'`, and `'Complete'`, and the last three elements of `shakes` to be `'William'`, `'Shakespeare'`, and `'\\x03'`.\n",
    "\n",
    "***Note***: If you are running into issues with opening the `'data/shakespeare.txt'` file, add the argument `encoding='UTF-8'` to the `open()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26416897773742676"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "import time\n",
    "start = time.time()\n",
    "shakes = tokenize(open(Path('data') / 'shakespeare.txt').read())\n",
    "elapsed = time.time() - start\n",
    "elapsed # Should be (much) under 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q2 results: All test cases passed!"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformLM(object):\n",
    "\n",
    "\n",
    "    def __init__(self, tokens):\n",
    "\n",
    "        self.mdl = self.train(tokens)\n",
    "        \n",
    "    def train(self, tokens):\n",
    "\n",
    "        unique = pd.Series(tokens).unique()\n",
    "\n",
    "        probabilities = [1/ len(unique)] * len(unique)\n",
    "        \n",
    "        return pd.Series(data=probabilities, index=unique)\n",
    "        \n",
    "    \n",
    "    def probability(self, words):\n",
    "\n",
    "        if not all(word in self.mdl for word in words):\n",
    "            return 0\n",
    "\n",
    "        probabilites = self.mdl.loc[list(words)]\n",
    "        \n",
    "        return float(probabilities.prod())\n",
    "        \n",
    "    def sample(self, M):\n",
    "        \n",
    "        probabilities = self.mdl.values\n",
    "\n",
    "        tokens = self.mdl.index\n",
    "\n",
    "        samples = np.random.choice(tokens, size=M, p=probabilities, replace=True)\n",
    "        \n",
    "        sentences = ' '.join(samples)\n",
    "        \n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one three two one three two two three four four four three three one three one two one four one one one four four one four four three two three two one two two two one four three four two two four two one one one two one four one two three one two one two four two three two two one three three three one two two two one two four three two one one one one two two one one two two three two two three one four two three four two three one two three one two'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "tokens = tuple('one one two three one two four'.split())\n",
    "unif = UniformLM(tokens)\n",
    "unif.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q3 results: All test cases passed!"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramLM(object):\n",
    "    \n",
    "    def __init__(self, tokens):\n",
    "        self.mdl = self.train(tokens)\n",
    "    \n",
    "    def train(self, tokens):\n",
    "        frequency = pd.Series(tokens).value_counts()\n",
    "        total = len(tokens)\n",
    "        return frequency / total\n",
    "    \n",
    "    def probability(self, words):\n",
    "        if not all(word in self.mdl for word in words): \n",
    "            return 0\n",
    "\n",
    "        word_probabilities = self.mdl.loc[list(words)]\n",
    "        return float(word_probabilities.prod())\n",
    "        \n",
    "    def sample(self, M):\n",
    "        vocabulary = self.mdl.index\n",
    "        probs = self.mdl.values\n",
    "        \n",
    "        selections = np.random.choice(vocabulary, size=M, p=probs, replace=True)\n",
    "        sentence = ' '.join(selections)\n",
    "        return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'two one two two one one two four one two two one one three two two two three one one one one one two three three one two one one one three one three one one two one one four four two two four two one four two three two two one one four one one two two four four one four two one two one one two one two four one one two one one one three two two one one one one one one two two two four three one one one one four two one two two'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "tokens = tuple('one one two three one two four'.split())\n",
    "unigram = UnigramLM(tokens)\n",
    "unigram.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q4</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q4 results: All test cases passed!"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Baseline Language Models\n",
    "\n",
    "You've now trained two baseline language models capable of generating new text from a given training text. Attempt to answer these questions for yourself before you continue.\n",
    "\n",
    "* Which model do you think is better? Why?\n",
    "* What are the ways in which both of these models are bad?\n",
    "\n",
    "If you haven't trained your models on the `shakes` corpus, uncomment and run the cells below to do so and generate new random \"sentences\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run â€“ should take less than 10 seconds\n",
    "shakes_uniform = UniformLM(shakes)\n",
    "shakes_unigram = UnigramLM(shakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'houseless Love safety unshaken tinctures resolutes keen blubbering Alban chronicles'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment and run\n",
    "shakes_uniform.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for Master a do Grace this out No all bondage'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment and run\n",
    "shakes_unigram.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a name='part3'></a>\n",
    "\n",
    "## Part 3: Creating an N-Gram Language Model ðŸ“š\n",
    "\n",
    "### Recap\n",
    "\n",
    "First, let's recap what we've done so far. Recall the chain rule for probability, where $w$ is a sentence made up of tokens $w_1, w_2, ..., w_n$:\n",
    "\n",
    "$$P(w) = P(w_1,\\ldots,w_n) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1,w_2) \\cdot\\ldots\\cdot P(w_n|w_1,\\ldots,w_{n-1})$$\n",
    "\n",
    "In Questions 3 (Uniform) and 4 (Unigram), your `train` methods computed these probabilities **unconditionally**, meaning that the probability that a token appeared in a sentence did **not depend** on the other tokens around it. That is, you said $P(w_i | w_1, w_2, ..., w_{i - 1}) = P(w_i)$. In Question 3, you let $P(w_i) = \\frac{1}{\\text{\\# unique tokens in corpus}}$, and in Question 4, you let $P(w_i) = \\frac{\\text{\\# tokens in corpus equal to $w_i$}}{\\text{\\# tokens in corpus}}$. Cruciually, each probability was determined by looking at the corpus that the model was trained on.\n",
    "\n",
    "<br>\n",
    "\n",
    "### N-Gram Overview\n",
    "\n",
    "Now we will build an N-Gram language model, in which the probability of a token appearing in a sentence **does depend** on the tokens that come before it. \n",
    "\n",
    "The chain rule above specifies that the probability that a token occurs at in a particular position in a sentence depends on **all** previous tokens in the sentence. However, it is often the case that the likelihood that a token appears in a sentence is influenced more by **nearby** tokens. (Remember, tokens are words, punctuation, or `'\\x02'` / `'\\x03'`).\n",
    "\n",
    "The N-Gram language model relies on the assumption that only nearby tokens matter. Specifically, it assumes that the probability that a token occurs depends only on the previous $N-1$ tokens, rather than all previous tokens. That is:\n",
    "\n",
    "$$P(w_n|w_1,\\ldots,w_{n-1}) = P(w_n|w_{n-(N-1)},\\ldots,w_{n-1})$$\n",
    "\n",
    "In an N-Gram language model, there is a hyperparameter that we get to choose when creating the model, $N$. For any $N$, the resulting N-Gram model looks at the previous $N-1$ tokens when computing probabilities. (Note that the unigram model you built in Question 4 is really an N-Gram model with $N=1$, since it looked at 0 previous tokens when computing probabilities.)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Example: Trigram Model\n",
    "\n",
    "When $N=3$, we have a \"trigram\" model. Such a model looks at the previous $N-1 = 2$ tokens when computing probabilities.\n",
    "\n",
    "Consider the tuple `('when', 'I', 'drink', 'Coke', 'I', 'smile')`, corresponding to the sentence `'when I drink Coke I smile'`. Under the trigram model, the probability of this sentence is computed as follows:\n",
    "\n",
    "$$P(\\text{when I drink Coke I smile}) = P(\\text{when}) \\cdot P(\\text{I | when}) \\cdot P(\\text{drink | when I}) \\cdot P(\\text{Coke | I drink}) \\cdot P(\\text{I | drink Coke}) \\cdot P(\\text{smile | Coke I})$$\n",
    "\n",
    "The trigram model doesn't consider the beginning of the sentence when computing the probability that the sentence ends in `'smile'`.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### N-Grams\n",
    "\n",
    "Both when working with a training corpus and when implementing the `probability` method to compute the probabilities of other sentences, you will need to work with \"chunks\" of $N$ tokens at a time.\n",
    "\n",
    "**Definition:** The **N-Grams of a text** are a list of tuples containing sliding windows of length $N$.\n",
    "\n",
    "For instance, the trigrams in the sentence `'when I drink Coke I smile'` are:\n",
    "\n",
    "```py\n",
    "[('when', 'I', 'drink'), ('I', 'drink', 'Coke'), ('drink', 'Coke', 'I'), ('Coke', 'I', 'smile')]\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Computing N-Gram Probabilities\n",
    "\n",
    "Notice in our trigram model above, we computed $P(\\text{when I drink Coke I smile})$ as being the product of several conditional probabilities. These conditional probabilities are the result of **training** our N-Gram model on a training corpus.\n",
    "\n",
    "To train an N-Gram model, we must compute a conditional probability for every $N$-token sequence in the corpus. For instance, suppose again that we are training a trigram model. Then, for every 3-token sequence $w_1, w_2, w_3$, we must compute $P(w_3 | w_1, w_2)$. To do so, we use:\n",
    "\n",
    "$$P(w_3 | w_1, w_2) = \\frac{C(w_1, w_2, w_3)}{C(w_1, w_2)}$$\n",
    "\n",
    "where $C(w_1, w_2, w_3)$ is the number of occurrences of the trigram sequence $w_1, w_2, w_3$ in the training corpus and $C(w_1, w_2)$ is the number of occurrences of the bigram sequence  $w_1, w_2$ in the training corpus. (Technical note: the probabilities that we compute using the ratios of counts are _estimates_ of the true conditional probabilities of N-Grams in the population of corpuses from which our corpus was drawn.)\n",
    "\n",
    "In general, for any $N$, conditional probabilities are computed by dividing the counts of N-Grams by the counts of the (N-1)-Grams they follow. \n",
    "\n",
    "**In the description of Question 5.2 we provide a detailed example of how we might compute such probabilities.**\n",
    "\n",
    "<br>\n",
    "\n",
    "### The `NGramLM` Class\n",
    "\n",
    "The `NGramLM` class contains a few extra methods and attributes beyond those of `UniformLM` and `UnigramLM`:\n",
    "\n",
    "1. Instantiating `NGramLM` requires both a list of tokens and a positive integer `N`, specifying the N in N-grams. This parameter is stored in an attribute `N`.\n",
    "    - Assume that `N` is less than or equal to `len(tokens)`.\n",
    "1. The `NGramLM` class has a method `create_ngrams` that takes in a list of tokens and returns a list of N-Grams (recall from above, an N-Gram is a **tuple** of length N). This list of N-Grams is then passed to the `train` method to train the N-Gram model.\n",
    "1. While the `train` method still creates a language model (in this case, an N-Gram model) and stores it in the `mdl` attribute, this model is most naturally stored as a DataFrame. This DataFrame will have three columns:\n",
    "    - `'ngram'`, containing the N-Grams found in the text.\n",
    "    - `'n1gram'`, containing the (N-1)-Grams upon which the N-Grams in `ngram` are built.\n",
    "    - `'prob'`, containing the probabilities of each N-Gram in `ngram`.\n",
    "1. The `NGramLM` class has an attribute `prev_mdl` that stores an (N-1)-Gram language model over the same corpus (which in turn will store an (N-2)-Gram language model over the same corpus, and so on). This is necessary to compute the probability that a word occurs at the start of a text. This code is included for you in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLM(object):\n",
    "    \n",
    "    def __init__(self, N, tokens):\n",
    "        # You don't need to edit the constructor,\n",
    "        # but you should understand how it works!\n",
    "        \n",
    "        self.N = N\n",
    "\n",
    "        ngrams = self.create_ngrams(tokens)\n",
    "\n",
    "        self.ngrams = ngrams\n",
    "        self.mdl = self.train(ngrams)\n",
    "\n",
    "        if N < 2:\n",
    "            raise Exception('N must be greater than 1')\n",
    "        elif N == 2:\n",
    "            self.prev_mdl = UnigramLM(tokens)\n",
    "        else:\n",
    "            self.prev_mdl = NGramLM(N-1, tokens)\n",
    "\n",
    "    def create_ngrams(self, tokens):\n",
    "        n = self.N\n",
    "        ngrams = []\n",
    "\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            ngrams.append(ngram)\n",
    "        return ngrams\n",
    "        \n",
    "    def train(self, ngrams):\n",
    "        # N-Gram counts C(w_1, ..., w_n)\n",
    "        ngram_counts = {}\n",
    "        for ngram in ngrams:\n",
    "            if ngram in ngram_counts:\n",
    "                ngram_counts[ngram] += 1\n",
    "            else:\n",
    "                ngram_counts[ngram] = 1\n",
    "        \n",
    "        # (N-1)-Gram counts C(w_1, ..., w_(n-1))\n",
    "        n_minus_one_counts = {}\n",
    "        for ngram, count in ngram_counts.items():\n",
    "            n_minus_one = ngram[:-1]\n",
    "            if n_minus_one in n_minus_one_counts:\n",
    "                n_minus_one_counts[n_minus_one] += 1\n",
    "            else:\n",
    "                n_minus_one_counts[n_minus_one] = 1\n",
    "\n",
    "\n",
    "        # Create the conditional probabilities\n",
    "        rows = []\n",
    "        for ngram, count in ngram_counts.items():\n",
    "            n_minus_one = ngram[:-1]\n",
    "            prob = count / n_minus_one_counts[n_minus_one]  # P(w_N | prefix)\n",
    "            rows.append((ngram, n_minus_one, prob))\n",
    "\n",
    "        df = pd.DataFrame(rows, columns=['ngram', 'n1gram', 'prob'])\n",
    "        return df\n",
    "\n",
    "    def probability(self, words):\n",
    "        ...\n",
    "    \n",
    "\n",
    "    def sample(self, M):\n",
    "        # Use a helper function to generate sample tokens of length `length`\n",
    "        ...\n",
    "        \n",
    "        # Transform the tokens to strings\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLM(object):\n",
    "    \n",
    "    def __init__(self, N, tokens):\n",
    "        self.N = N\n",
    "        ngrams = self.create_ngrams(tokens)\n",
    "        self.ngrams = ngrams\n",
    "        self.mdl = self.train(ngrams)\n",
    "        \n",
    "        if N < 2:\n",
    "            raise Exception('N must be greater than 1')\n",
    "        elif N == 2:\n",
    "            self.prev_mdl = UnigramLM(tokens)\n",
    "        else:\n",
    "            self.prev_mdl = NGramLM(N - 1, tokens)\n",
    "    \n",
    "    def create_ngrams(self, tokens):\n",
    "        n = self.N\n",
    "        ngrams = []\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngram = tuple(tokens[i:i+n])\n",
    "            ngrams.append(ngram)\n",
    "        return ngrams\n",
    "        \n",
    "    def train(self, ngrams):\n",
    "        # Count each full N-gram.\n",
    "        ngram_counts = {}\n",
    "        for ngram in ngrams:\n",
    "            ngram_counts[ngram] = ngram_counts.get(ngram, 0) + 1\n",
    "        \n",
    "        # Count each (N-1)-gram prefix.\n",
    "        n_minus_one_counts = {}\n",
    "        for ngram, count in ngram_counts.items():\n",
    "            prefix = ngram[:-1]\n",
    "            n_minus_one_counts[prefix] = n_minus_one_counts.get(prefix, 0) + count\n",
    "        \n",
    "        # Create rows with conditional probabilities: \n",
    "        # P(w_N | prefix) = count(ngram) / count(prefix)\n",
    "        rows = []\n",
    "        for ngram, count in ngram_counts.items():\n",
    "            prefix = ngram[:-1]\n",
    "            prob = count / n_minus_one_counts[prefix]\n",
    "            rows.append((ngram, prefix, prob))\n",
    "        df = pd.DataFrame(rows, columns=['ngram', 'n1gram', 'prob'])\n",
    "        return df\n",
    "    \n",
    "    def probability(self, sentence):\n",
    "        if self.N == 1:\n",
    "            prob = 1.0\n",
    "            for word in sentence:\n",
    "                row = self.mdl[self.mdl['ngram'] == (word,)]\n",
    "                if row.empty:\n",
    "                    return 0\n",
    "                prob *= row['prob'].iloc[0]\n",
    "            return prob\n",
    "\n",
    "        initial_history = sentence[:self.N - 1]\n",
    "        prob = self.prev_mdl.probability(initial_history)\n",
    "        \n",
    "        for i in range(len(sentence) - self.N + 1):\n",
    "            current_ngram = tuple(sentence[i:i+self.N])\n",
    "            row = self.mdl[self.mdl['ngram'] == current_ngram]\n",
    "            if not row.empty:\n",
    "                prob *= row['prob'].iloc[0]\n",
    "            else:\n",
    "                if self.prev_mdl:\n",
    "                    prob *= self.prev_mdl.probability(current_ngram[:-1])\n",
    "                else:\n",
    "                    return 0\n",
    "        return prob\n",
    "\n",
    "    def sample(self, M):\n",
    "        tokens = ['\\x02']  \n",
    "        while (len(tokens) - 1) < M:\n",
    "            context = tuple(tokens[-(self.N - 1):]) if len(tokens) >= (self.N - 1) else tuple(tokens)\n",
    "            \n",
    "            candidate_rows = self.mdl[self.mdl['n1gram'] == context]\n",
    "\n",
    "            if candidate_rows.empty or ((len(tokens) - 1) == (M - 1)):\n",
    "                tokens.append('\\x03')\n",
    "                break\n",
    "            \n",
    "            candidate_tokens = candidate_rows['ngram'].apply(lambda x: x[-1]).tolist()\n",
    "            candidate_probs = candidate_rows['prob'].tolist()\n",
    "            \n",
    "            candidate_probs = np.array(candidate_probs, dtype=float)\n",
    "            candidate_probs = candidate_probs / candidate_probs.sum()\n",
    "            \n",
    "            next_token = np.random.choice(candidate_tokens, p=candidate_probs)\n",
    "            tokens.append(next_token)\n",
    "        \n",
    "        return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "tokens = tuple('\\x02 one two three one four \\x03'.split())\n",
    "bigrams = NGramLM(2, tokens)\n",
    "out = bigrams.create_ngrams(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell, but do run it -- it is needed for the tests\n",
    "tokens = \"\\x02 Humpty Dumpty sat on a wall , Humpty Dumpty had a great fall . \\x03 \\x02 All the king ' s horses and all the king ' s men couldn ' t put Humpty together again . \\x03\".split()\n",
    "tokens = tuple(tokens)\n",
    "ngram = NGramLM(2, tokens)\n",
    "out_5a1 = ngram.create_ngrams(tokens)\n",
    "out_5b1 = ngram.mdl\n",
    "out_5c1 = ngram\n",
    "out_5d1 = ngram.sample(500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\x02', 'Humpty'),\n",
       " ('Humpty', 'Dumpty'),\n",
       " ('Dumpty', 'sat'),\n",
       " ('sat', 'on'),\n",
       " ('on', 'a'),\n",
       " ('a', 'wall'),\n",
       " ('wall', ','),\n",
       " (',', 'Humpty'),\n",
       " ('Humpty', 'Dumpty'),\n",
       " ('Dumpty', 'had'),\n",
       " ('had', 'a'),\n",
       " ('a', 'great'),\n",
       " ('great', 'fall'),\n",
       " ('fall', '.'),\n",
       " ('.', '\\x03'),\n",
       " ('\\x03', '\\x02'),\n",
       " ('\\x02', 'All'),\n",
       " ('All', 'the'),\n",
       " ('the', 'king'),\n",
       " ('king', \"'\"),\n",
       " (\"'\", 's'),\n",
       " ('s', 'horses'),\n",
       " ('horses', 'and'),\n",
       " ('and', 'all'),\n",
       " ('all', 'the'),\n",
       " ('the', 'king'),\n",
       " ('king', \"'\"),\n",
       " (\"'\", 's'),\n",
       " ('s', 'men'),\n",
       " ('men', 'couldn'),\n",
       " ('couldn', \"'\"),\n",
       " (\"'\", 't'),\n",
       " ('t', 'put'),\n",
       " ('put', 'Humpty'),\n",
       " ('Humpty', 'together'),\n",
       " ('together', 'again'),\n",
       " ('again', '.'),\n",
       " ('.', '\\x03')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_5a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>n1gram</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(\u0002, Humpty)</td>\n",
       "      <td>(\u0002,)</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Humpty, Dumpty)</td>\n",
       "      <td>(Humpty,)</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Dumpty, sat)</td>\n",
       "      <td>(Dumpty,)</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(sat, on)</td>\n",
       "      <td>(sat,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(on, a)</td>\n",
       "      <td>(on,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(a, wall)</td>\n",
       "      <td>(a,)</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(wall, ,)</td>\n",
       "      <td>(wall,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(,, Humpty)</td>\n",
       "      <td>(,,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Dumpty, had)</td>\n",
       "      <td>(Dumpty,)</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(had, a)</td>\n",
       "      <td>(had,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(a, great)</td>\n",
       "      <td>(a,)</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(great, fall)</td>\n",
       "      <td>(great,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(fall, .)</td>\n",
       "      <td>(fall,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(., \u0003)</td>\n",
       "      <td>(.,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(\u0003, \u0002)</td>\n",
       "      <td>(\u0003,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(\u0002, All)</td>\n",
       "      <td>(\u0002,)</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(All, the)</td>\n",
       "      <td>(All,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(the, king)</td>\n",
       "      <td>(the,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(king, ')</td>\n",
       "      <td>(king,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(', s)</td>\n",
       "      <td>(',)</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(s, horses)</td>\n",
       "      <td>(s,)</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(horses, and)</td>\n",
       "      <td>(horses,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(and, all)</td>\n",
       "      <td>(and,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(all, the)</td>\n",
       "      <td>(all,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(s, men)</td>\n",
       "      <td>(s,)</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(men, couldn)</td>\n",
       "      <td>(men,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(couldn, ')</td>\n",
       "      <td>(couldn,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(', t)</td>\n",
       "      <td>(',)</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(t, put)</td>\n",
       "      <td>(t,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(put, Humpty)</td>\n",
       "      <td>(put,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(Humpty, together)</td>\n",
       "      <td>(Humpty,)</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(together, again)</td>\n",
       "      <td>(together,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(again, .)</td>\n",
       "      <td>(again,)</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ngram       n1gram      prob\n",
       "0          (\u0002, Humpty)         (\u0002,)  0.500000\n",
       "1     (Humpty, Dumpty)    (Humpty,)  0.666667\n",
       "2        (Dumpty, sat)    (Dumpty,)  0.500000\n",
       "3            (sat, on)       (sat,)  1.000000\n",
       "4              (on, a)        (on,)  1.000000\n",
       "5            (a, wall)         (a,)  0.500000\n",
       "6            (wall, ,)      (wall,)  1.000000\n",
       "7          (,, Humpty)         (,,)  1.000000\n",
       "8        (Dumpty, had)    (Dumpty,)  0.500000\n",
       "9             (had, a)       (had,)  1.000000\n",
       "10          (a, great)         (a,)  0.500000\n",
       "11       (great, fall)     (great,)  1.000000\n",
       "12           (fall, .)      (fall,)  1.000000\n",
       "13              (., \u0003)         (.,)  1.000000\n",
       "14              (\u0003, \u0002)         (\u0003,)  1.000000\n",
       "15            (\u0002, All)         (\u0002,)  0.500000\n",
       "16          (All, the)       (All,)  1.000000\n",
       "17         (the, king)       (the,)  1.000000\n",
       "18           (king, ')      (king,)  1.000000\n",
       "19              (', s)         (',)  0.666667\n",
       "20         (s, horses)         (s,)  0.500000\n",
       "21       (horses, and)    (horses,)  1.000000\n",
       "22          (and, all)       (and,)  1.000000\n",
       "23          (all, the)       (all,)  1.000000\n",
       "24            (s, men)         (s,)  0.500000\n",
       "25       (men, couldn)       (men,)  1.000000\n",
       "26         (couldn, ')    (couldn,)  1.000000\n",
       "27              (', t)         (',)  0.333333\n",
       "28            (t, put)         (t,)  1.000000\n",
       "29       (put, Humpty)       (put,)  1.000000\n",
       "30  (Humpty, together)    (Humpty,)  0.333333\n",
       "31   (together, again)  (together,)  1.000000\n",
       "32          (again, .)     (again,)  1.000000"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_5b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q5</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q5 results: All test cases passed!"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've built an N-Gram language model, let's use it to actually generate sentences!\n",
    "\n",
    "Uncomment and run the cell below to define a bigram model using the `shakes` corpus and to generate a sentence of length 50 using the model. **The cell should run in under 30 seconds.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x02 TRANIO . \\x03 \\x02 PETRUCHIO . I do this May show too , The son jurement de Englishman . Strong Enobarb Is the King , To me of all of riches ; By the rest you . Him I will spend in disdain , this , and let the \\x03'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment and run\n",
    "shakes_bigram = NGramLM(2, shakes)\n",
    "shakes_bigram.sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hidden tests in Part 3/Question 5 will test your `NGramLM` implementation on corpuses that are much longer than the public tests. One of the corpuses we will test your implementation on is in `data/homertokens.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "homer_tokens = tuple(open(Path('data') / 'homertokens.txt').read().split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As such, it's a good idea to make sure that you can instantiate an `NGramLM` object using `homer_tokens` and that all methods (`probability`, `sample`) run in under ~20 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NGramLM at 0x138f40950>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NGramLM(5, homer_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations, you've finished Project 3! ðŸŽ‰\n",
    "\n",
    "As a reminder, all of the work you want to submit needs to be in `project.py`.\n",
    "\n",
    "To ensure that all of the work you want to submit is in `project.py`, we've included a script named `project-validation.py` in the project folder. You shouldn't edit it, but instead, you should call it from the command line (e.g. the Terminal) to test your work.\n",
    "\n",
    "Once you've finished the project, you should open the command line and run, in the directory for this project:\n",
    "\n",
    "```\n",
    "python project-validation.py\n",
    "```\n",
    "\n",
    "**This will run all of the `grader.check` cells that you see in this notebook, but only using the code in `project.py` â€“ that is, it doesn't look at any of the code in this notebook. If all of your `grader.check` cells pass in this notebook but not all of them pass in your command line with the above command, then you likely have code in your notebook that isn't in your `project.py`!**\n",
    "\n",
    "You can also use `project-validation.py` to test individual questions. For instance,\n",
    "\n",
    "```\n",
    "python project-validation.py q1 q4\n",
    "```\n",
    "\n",
    "will run the `grader.check` cells for Questions 1 and 4 â€“ again, only using the code in `project.py`.\n",
    "\n",
    "Once `python project-validation.py` shows that you're passing all test cases, you're ready to submit your `project.py` (and only your `project.py`) to Gradescope. Once submitting to Gradescope, make sure to stick around until all test cases pass.\n",
    "\n",
    "There is also a call to `grader.check_all()` below in _this_ notebook, but make sure to also follow the steps above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q1 results: All test cases passed!\n",
       "\n",
       "q2 results: All test cases passed!\n",
       "\n",
       "q3 results: All test cases passed!\n",
       "\n",
       "q4 results: All test cases passed!\n",
       "\n",
       "q5 results: All test cases passed!"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(beowulf, str)\nTrue",
         "failure_message": "check type to make sure the hidden tests will work",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> # Why two possibilities?\n>>> # The book changed since the last quarter we offered the project;\n>>> # we're looking for the first output, but just in case the book changes again\n>>> # while you're working on the project, we've provided the old expected output too.\n>>> beowulf[:20] in ['\\n\\n\\n\\n\\nBEOWULF\\nAN ANGL', '\\n\\n\\n\\n\\nProduced by Dav']\nTrue",
         "failure_message": "make sure front matter not in string",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> 1000000 <= len(shakes) <= 1500000\nTrue",
         "failure_message": "approx correct number of tokens",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> elapsed <= 5\nTrue",
         "failure_message": "shakespeare fast enough",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> shakes[:3] == ['\\x02', 'The', 'Complete'] and shakes[-3:] == ['William', 'Shakespeare', '\\x03']\nTrue",
         "failure_message": "check beginning and ending",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool((unif.mdl == 0.25).all())\nTrue",
         "failure_message": "only one probability",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> unif.mdl.shape[0] == 4\nTrue",
         "failure_message": "number of tokens in mdl",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(unif.mdl, pd.Series)\nTrue",
         "failure_message": "mdl correct type",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> set(unif.mdl.index) == set('one two three four'.split())\nTrue",
         "failure_message": "correct indices for mdl",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> unif.probability(('five',)) == 0\nTrue",
         "failure_message": "five not a token",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> unif.probability(('one', 'two')) == 0.0625\nTrue",
         "failure_message": "probability of given sequence appearing",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> isinstance(unif.sample(1000), str)\nTrue",
         "failure_message": "sample is string",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> len(unif.sample(1000).split()) == 1000\nTrue",
         "failure_message": "length of sample",
         "hidden": false,
         "locked": false,
         "points": 0.25
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> unigram.mdl.nunique() == 3\nTrue",
         "failure_message": "test mdl",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> isinstance(unigram.mdl, pd.Series)\nTrue",
         "failure_message": "mdl correct type",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> set(unigram.mdl.index) == set('one two three four'.split())\nTrue",
         "failure_message": "correct indices for mdl",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> unigram.probability(('five',)) == 0\nTrue",
         "failure_message": "five not a token",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> isinstance(unigram.sample(1000), str)\nTrue",
         "failure_message": "sample is string",
         "hidden": false,
         "locked": false,
         "points": 0.25
        },
        {
         "code": ">>> len(unigram.sample(1000).split()) == 1000\nTrue",
         "failure_message": "sample length",
         "hidden": false,
         "locked": false,
         "points": 0.25
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(out[0], tuple)\nTrue",
         "failure_message": "test create_ngrams",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> out[0] == ('\\x02', 'one') and out[2] == ('two', 'three')\nTrue",
         "failure_message": "test create_ngrams",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> set(bigrams.mdl.columns) == set('ngram n1gram prob'.split())\nTrue",
         "failure_message": "test train",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> bool(bigrams.mdl.shape == (6, 3) and bigrams.mdl['prob'].min() == 0.5)\nTrue",
         "failure_message": "test train",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> tokens = tuple('\\x02 one two one three one two \\x03'.split())\n>>> bigrams2 = NGramLM(2, tokens)\n>>> p = bigrams2.probability('two one three'.split())\n>>> bool(np.isclose(p, (1/4) * (1/2) * (1/3), 0.01))\nTrue",
         "failure_message": "test probability",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> samp = bigrams.sample(3)\n>>> len(samp.split()) == 4  # don't count the initial START token.\nTrue",
         "failure_message": "test sample",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> samp = bigrams.sample(3)\n>>> samp[:2] == '\\x02 ' and  set(samp.split()) <= {'\\x02', '\\x03', 'one', 'two', 'three', 'four'}\nTrue",
         "failure_message": "test sample",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> 36 <= len(out_5a1) <= 40 and all([len(x) == 2 for x in out_5a1]) and out_5a1[0] == ('\\x02', 'Humpty')\nTrue",
         "failure_message": "test sample",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> #Older test cases (python 3.8)\n>>> #np.isclose(out_5b1.groupby('n1gram').sum(), 1.0, 1.0).all() and (out_5b1['n1gram'].apply(len) == 1).all()\n>>> bool(np.isclose(out_5b1.groupby('n1gram')['prob'].sum(),1.0).all() and (out_5b1['n1gram'].apply(len) == 1).all())\nTrue",
         "failure_message": "test sample",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> (out_5b1['ngram'].apply(len) == 2).all() and \\\n... bool(np.isclose(out_5b1.loc[out_5b1.ngram == ('Humpty', 'Dumpty'), 'prob'].squeeze(), 0.666666, atol=0.1))\nTrue",
         "failure_message": "test sample",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> bool(np.isclose(out_5c1.probability('Humpty Dumpty sat on a great fall'.split()), 0.01282051282051282, atol=0.1))\nTrue",
         "failure_message": "test sample",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> vc = pd.Series(out_5d1.split()).value_counts(normalize=True)\n>>> {'\\'', 'the', 'Humpty'}.intersection(set(vc.index[:6])) != {}\nTrue",
         "failure_message": "most common sampled tokens",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
